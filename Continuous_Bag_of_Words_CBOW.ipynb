{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fb4a20-bbf0-487d-8c1c-55f1b9852d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tmodel_storage/ch5/cbow\\vectorizer.json\n",
      "\tmodel_storage/ch5/cbow\\model.pth\n",
      "Using CUDA: False\n",
      "Loading dataset and creating vectorizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training routine:   0%|                                                                          | 0/5 [00:00<?, ?it/s]\n",
      "\u001b[Ait=train:   0%|                                                                            | 0/1984 [00:00<?, ?it/s]\n",
      "\u001b[Ait=val:   0%|                                                                               | 0/425 [00:00<?, ?it/s]\n",
      "\u001b[Ait=train:   0%|                                                   | 0/1984 [00:00<?, ?it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   0%|▏                                          | 6/1984 [00:00<00:33, 58.29it/s, acc=0, epoch=0, loss=10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[Ait=train:   1%|▎                                         | 16/1984 [00:00<00:24, 81.71it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   1%|▌                                         | 25/1984 [00:00<00:23, 85.08it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   2%|▋                                         | 35/1984 [00:00<00:22, 88.02it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   2%|▉                                         | 45/1984 [00:00<00:21, 90.95it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   3%|█▏                                        | 55/1984 [00:00<00:21, 89.37it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   3%|█▎                                        | 64/1984 [00:00<00:21, 89.36it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   4%|█▌                                        | 74/1984 [00:00<00:20, 92.31it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   4%|█▊                                        | 85/1984 [00:00<00:20, 94.64it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   5%|██                                        | 95/1984 [00:01<00:20, 90.85it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   5%|██▏                                      | 105/1984 [00:01<00:20, 89.96it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   6%|██▍                                      | 115/1984 [00:01<00:20, 90.17it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   6%|██▌                                      | 125/1984 [00:01<00:20, 90.74it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   7%|██▊                                      | 135/1984 [00:01<00:19, 92.62it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   7%|██▉                                      | 145/1984 [00:01<00:19, 93.20it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   8%|███▏                                     | 156/1984 [00:01<00:19, 95.48it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   8%|███▍                                     | 166/1984 [00:01<00:19, 92.44it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   9%|███▋                                     | 176/1984 [00:01<00:19, 94.07it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:   9%|███▊                                     | 186/1984 [00:02<00:18, 95.31it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  10%|████                                     | 196/1984 [00:02<00:20, 88.63it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  10%|████▏                                    | 205/1984 [00:02<00:20, 88.84it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  11%|████▍                                    | 214/1984 [00:02<00:19, 89.00it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  11%|████▋                                    | 224/1984 [00:02<00:19, 90.06it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  12%|████▊                                    | 234/1984 [00:02<00:19, 91.14it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  12%|█████                                    | 244/1984 [00:02<00:18, 91.70it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  13%|█████▏                                   | 254/1984 [00:02<00:18, 93.18it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  13%|█████▍                                   | 264/1984 [00:02<00:18, 94.28it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  14%|█████▋                                   | 274/1984 [00:03<00:18, 93.84it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  14%|█████▊                                   | 284/1984 [00:03<00:17, 95.05it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  15%|██████                                   | 294/1984 [00:03<00:17, 95.01it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  15%|██████▎                                  | 304/1984 [00:03<00:17, 96.17it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  16%|██████▍                                  | 314/1984 [00:03<00:17, 95.85it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  16%|██████▋                                  | 325/1984 [00:03<00:16, 98.27it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  17%|██████▊                                 | 336/1984 [00:03<00:16, 100.55it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  17%|██████▉                                 | 347/1984 [00:03<00:16, 101.08it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  18%|███████▏                                | 358/1984 [00:03<00:16, 101.15it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  19%|███████▍                                | 369/1984 [00:03<00:16, 100.55it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  19%|███████▊                                 | 380/1984 [00:04<00:16, 95.77it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  20%|████████                                 | 390/1984 [00:04<00:16, 96.32it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  20%|████████▎                                | 400/1984 [00:04<00:16, 97.23it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  21%|████████▍                                | 410/1984 [00:04<00:16, 97.38it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  21%|████████▋                                | 421/1984 [00:04<00:15, 98.13it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  22%|████████▉                                | 431/1984 [00:04<00:16, 95.00it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  22%|█████████                                | 441/1984 [00:04<00:16, 95.43it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  23%|█████████▎                               | 452/1984 [00:04<00:15, 97.69it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  23%|█████████▌                               | 462/1984 [00:04<00:15, 96.05it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  24%|█████████▊                               | 472/1984 [00:05<00:16, 94.04it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  24%|█████████▉                               | 482/1984 [00:05<00:16, 93.36it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  25%|██████████▏                              | 492/1984 [00:05<00:15, 93.94it/s, acc=0, epoch=0, loss=10]\n",
      "\u001b[Ait=train:  25%|████████▊                          | 500/1984 [00:05<00:15, 93.94it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  25%|████████▊                          | 502/1984 [00:05<00:15, 94.91it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  26%|█████████                          | 512/1984 [00:05<00:15, 96.13it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  26%|█████████▏                         | 523/1984 [00:05<00:14, 97.80it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  27%|█████████▍                         | 534/1984 [00:05<00:14, 97.57it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  27%|█████████▌                         | 544/1984 [00:05<00:14, 97.32it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  28%|█████████▊                         | 554/1984 [00:05<00:14, 97.25it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  28%|█████████▉                         | 565/1984 [00:05<00:14, 98.39it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  29%|██████████▏                        | 575/1984 [00:06<00:14, 98.28it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  29%|██████████▎                        | 585/1984 [00:06<00:14, 97.19it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  30%|██████████▍                        | 595/1984 [00:06<00:14, 95.98it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  30%|██████████▋                        | 605/1984 [00:06<00:14, 96.70it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  31%|██████████▊                        | 615/1984 [00:06<00:14, 96.32it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  32%|███████████                        | 625/1984 [00:06<00:13, 97.20it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  32%|███████████▏                       | 635/1984 [00:06<00:14, 95.71it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  33%|███████████▍                       | 646/1984 [00:06<00:13, 97.54it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  33%|███████████▌                       | 657/1984 [00:06<00:13, 99.59it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  34%|███████████▊                       | 667/1984 [00:07<00:14, 92.83it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  34%|███████████▉                       | 678/1984 [00:07<00:13, 94.70it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  35%|████████████▏                      | 688/1984 [00:07<00:13, 94.39it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  35%|████████████▎                      | 698/1984 [00:07<00:13, 95.46it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  36%|████████████▍                      | 708/1984 [00:07<00:13, 96.64it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  36%|████████████▋                      | 718/1984 [00:07<00:12, 97.46it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  37%|████████████▊                      | 728/1984 [00:07<00:12, 96.74it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  37%|█████████████                      | 738/1984 [00:07<00:13, 95.66it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  38%|█████████████▏                     | 749/1984 [00:07<00:12, 97.58it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  38%|█████████████▍                     | 759/1984 [00:08<00:12, 97.19it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  39%|█████████████▌                     | 769/1984 [00:08<00:12, 97.14it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  39%|█████████████▋                     | 779/1984 [00:08<00:12, 97.46it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  40%|█████████████▌                    | 790/1984 [00:08<00:11, 100.28it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  40%|██████████████▏                    | 801/1984 [00:08<00:12, 93.05it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  41%|██████████████▎                    | 811/1984 [00:08<00:12, 94.60it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  41%|██████████████▍                    | 821/1984 [00:08<00:12, 95.74it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  42%|██████████████▋                    | 831/1984 [00:08<00:11, 96.39it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  42%|██████████████▊                    | 842/1984 [00:08<00:11, 98.72it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  43%|███████████████                    | 852/1984 [00:08<00:11, 98.18it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  43%|███████████████▏                   | 862/1984 [00:09<00:11, 94.88it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  44%|███████████████▍                   | 872/1984 [00:09<00:11, 93.48it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  44%|███████████████▌                   | 882/1984 [00:09<00:11, 92.52it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  45%|███████████████▋                   | 892/1984 [00:09<00:12, 88.79it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  45%|███████████████▉                   | 901/1984 [00:09<00:12, 87.35it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  46%|████████████████                   | 912/1984 [00:09<00:11, 91.31it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  46%|████████████████▎                  | 922/1984 [00:09<00:11, 91.97it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  47%|████████████████▍                  | 932/1984 [00:09<00:11, 92.31it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  47%|████████████████▌                  | 942/1984 [00:09<00:11, 94.40it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  48%|████████████████▊                  | 952/1984 [00:10<00:10, 95.06it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  48%|████████████████▉                  | 962/1984 [00:10<00:10, 95.18it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  49%|█████████████████▏                 | 973/1984 [00:10<00:10, 97.52it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  50%|█████████████████▎                 | 983/1984 [00:10<00:10, 97.39it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  50%|█████████████████▌                 | 993/1984 [00:10<00:10, 95.89it/s, acc=0.181, epoch=0, loss=9.57]\n",
      "\u001b[Ait=train:  50%|█████████████████▋                 | 1000/1984 [00:10<00:10, 95.89it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  51%|█████████████████▋                 | 1003/1984 [00:10<00:10, 95.11it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  51%|█████████████████▊                 | 1013/1984 [00:10<00:10, 94.05it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  52%|██████████████████                 | 1023/1984 [00:10<00:10, 92.57it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  52%|██████████████████▏                | 1033/1984 [00:10<00:10, 93.03it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  53%|██████████████████▍                | 1043/1984 [00:11<00:10, 92.51it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  53%|██████████████████▌                | 1053/1984 [00:11<00:10, 90.93it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  54%|██████████████████▊                | 1063/1984 [00:11<00:10, 91.36it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  54%|██████████████████▉                | 1073/1984 [00:11<00:10, 87.29it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  55%|███████████████████                | 1083/1984 [00:11<00:10, 88.81it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  55%|███████████████████▎               | 1093/1984 [00:11<00:09, 90.57it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  56%|███████████████████▍               | 1103/1984 [00:11<00:09, 92.04it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  56%|███████████████████▋               | 1113/1984 [00:11<00:09, 93.77it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  57%|███████████████████▊               | 1124/1984 [00:11<00:08, 96.27it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  57%|████████████████████               | 1134/1984 [00:12<00:08, 95.59it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  58%|████████████████████▏              | 1144/1984 [00:12<00:08, 95.45it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  58%|████████████████████▎              | 1154/1984 [00:12<00:08, 94.76it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  59%|████████████████████▌              | 1164/1984 [00:12<00:08, 95.27it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  59%|████████████████████▋              | 1174/1984 [00:12<00:08, 95.53it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  60%|████████████████████▉              | 1184/1984 [00:12<00:08, 96.64it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  60%|█████████████████████              | 1194/1984 [00:12<00:08, 97.45it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  61%|█████████████████████▏             | 1204/1984 [00:12<00:07, 97.72it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  61%|█████████████████████▍             | 1214/1984 [00:12<00:08, 93.01it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  62%|█████████████████████▌             | 1224/1984 [00:12<00:08, 92.98it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  62%|█████████████████████▊             | 1234/1984 [00:13<00:08, 93.61it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  63%|█████████████████████▉             | 1244/1984 [00:13<00:08, 91.52it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  63%|██████████████████████             | 1254/1984 [00:13<00:07, 92.18it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  64%|██████████████████████▎            | 1265/1984 [00:13<00:07, 94.88it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  64%|██████████████████████▍            | 1275/1984 [00:13<00:07, 95.81it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  65%|██████████████████████▋            | 1285/1984 [00:13<00:07, 96.54it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  65%|██████████████████████▊            | 1296/1984 [00:13<00:06, 98.68it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  66%|███████████████████████            | 1306/1984 [00:13<00:06, 98.29it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  66%|███████████████████████▏           | 1316/1984 [00:13<00:06, 98.57it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  67%|███████████████████████▍           | 1326/1984 [00:14<00:06, 97.29it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  67%|███████████████████████▌           | 1336/1984 [00:14<00:06, 95.59it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  68%|███████████████████████▋           | 1346/1984 [00:14<00:06, 96.19it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  68%|███████████████████████▉           | 1356/1984 [00:14<00:06, 94.98it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  69%|████████████████████████           | 1367/1984 [00:14<00:06, 97.09it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  69%|████████████████████████▎          | 1378/1984 [00:14<00:06, 98.54it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  70%|████████████████████████▍          | 1388/1984 [00:14<00:06, 97.70it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  70%|████████████████████████▋          | 1398/1984 [00:14<00:06, 97.14it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  71%|████████████████████████▊          | 1408/1984 [00:14<00:06, 94.02it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  72%|█████████████████████████          | 1419/1984 [00:14<00:05, 96.04it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  72%|█████████████████████████▏         | 1430/1984 [00:15<00:05, 97.21it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  73%|█████████████████████████▍         | 1440/1984 [00:15<00:05, 90.67it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  73%|█████████████████████████▌         | 1450/1984 [00:15<00:05, 90.75it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  74%|█████████████████████████▊         | 1460/1984 [00:15<00:05, 90.30it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  74%|█████████████████████████▉         | 1470/1984 [00:15<00:05, 92.84it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  75%|██████████████████████████         | 1480/1984 [00:15<00:05, 94.58it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  75%|██████████████████████████▎        | 1491/1984 [00:15<00:05, 96.60it/s, acc=0.45, epoch=0, loss=9.23]\n",
      "\u001b[Ait=train:  76%|██████████████████████████▍        | 1500/1984 [00:15<00:05, 96.60it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  76%|██████████████████████████▍        | 1501/1984 [00:15<00:04, 97.26it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  76%|██████████████████████████▋        | 1511/1984 [00:15<00:04, 96.55it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  77%|██████████████████████████▊        | 1521/1984 [00:16<00:04, 96.73it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  77%|███████████████████████████        | 1531/1984 [00:16<00:04, 94.84it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  78%|███████████████████████████▏       | 1541/1984 [00:16<00:04, 95.94it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  78%|███████████████████████████▎       | 1551/1984 [00:16<00:04, 95.17it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  79%|███████████████████████████▌       | 1561/1984 [00:16<00:04, 94.76it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  79%|███████████████████████████▋       | 1571/1984 [00:16<00:04, 94.22it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  80%|███████████████████████████▉       | 1581/1984 [00:16<00:04, 91.95it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  80%|████████████████████████████       | 1591/1984 [00:16<00:04, 89.92it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  81%|████████████████████████████▏      | 1601/1984 [00:16<00:04, 91.86it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  81%|████████████████████████████▍      | 1611/1984 [00:17<00:04, 92.79it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  82%|████████████████████████████▌      | 1621/1984 [00:17<00:03, 91.65it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  82%|████████████████████████████▊      | 1631/1984 [00:17<00:03, 93.34it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  83%|████████████████████████████▉      | 1641/1984 [00:17<00:03, 91.88it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  83%|█████████████████████████████▏     | 1651/1984 [00:17<00:03, 94.01it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  84%|█████████████████████████████▎     | 1661/1984 [00:17<00:03, 93.20it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  84%|█████████████████████████████▍     | 1671/1984 [00:17<00:03, 94.65it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  85%|█████████████████████████████▋     | 1681/1984 [00:17<00:03, 94.20it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  85%|█████████████████████████████▊     | 1691/1984 [00:17<00:03, 94.38it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  86%|██████████████████████████████     | 1701/1984 [00:18<00:03, 94.10it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  86%|██████████████████████████████▏    | 1711/1984 [00:18<00:02, 93.86it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  87%|██████████████████████████████▎    | 1721/1984 [00:18<00:02, 94.40it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  87%|██████████████████████████████▌    | 1731/1984 [00:18<00:02, 92.57it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  88%|██████████████████████████████▋    | 1741/1984 [00:18<00:02, 93.19it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  88%|██████████████████████████████▉    | 1751/1984 [00:18<00:02, 94.21it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  89%|███████████████████████████████    | 1761/1984 [00:18<00:02, 93.04it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  89%|███████████████████████████████▏   | 1771/1984 [00:18<00:02, 94.29it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  90%|███████████████████████████████▍   | 1781/1984 [00:18<00:02, 93.71it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  90%|███████████████████████████████▌   | 1791/1984 [00:18<00:02, 91.76it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  91%|███████████████████████████████▊   | 1801/1984 [00:19<00:02, 88.96it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  91%|███████████████████████████████▉   | 1810/1984 [00:19<00:01, 88.16it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  92%|████████████████████████████████   | 1819/1984 [00:19<00:01, 88.67it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  92%|████████████████████████████████▏  | 1828/1984 [00:19<00:01, 88.60it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  93%|████████████████████████████████▍  | 1838/1984 [00:19<00:01, 91.07it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  93%|████████████████████████████████▌  | 1848/1984 [00:19<00:01, 91.73it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  94%|████████████████████████████████▊  | 1858/1984 [00:19<00:01, 92.44it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  94%|████████████████████████████████▉  | 1868/1984 [00:19<00:01, 92.88it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  95%|█████████████████████████████████▏ | 1878/1984 [00:19<00:01, 93.87it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  95%|█████████████████████████████████▎ | 1888/1984 [00:20<00:01, 90.86it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  96%|█████████████████████████████████▍ | 1898/1984 [00:20<00:01, 85.71it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  96%|█████████████████████████████████▋ | 1908/1984 [00:20<00:00, 87.35it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  97%|█████████████████████████████████▊ | 1917/1984 [00:20<00:00, 87.80it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  97%|█████████████████████████████████▉ | 1927/1984 [00:20<00:00, 90.47it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  98%|██████████████████████████████████▏| 1937/1984 [00:20<00:00, 91.40it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  98%|██████████████████████████████████▎| 1947/1984 [00:20<00:00, 88.99it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  99%|██████████████████████████████████▌| 1957/1984 [00:20<00:00, 90.85it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train:  99%|██████████████████████████████████▋| 1967/1984 [00:20<00:00, 90.72it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=train: 100%|██████████████████████████████████▉| 1977/1984 [00:21<00:00, 92.55it/s, acc=0.92, epoch=0, loss=8.99]\n",
      "\u001b[Ait=val:   0%|                                                 | 0/425 [00:21<?, ?it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:   0%|                                       | 1/425 [00:21<2:29:11, 21.11s/it, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:   6%|██▎                                     | 25/425 [00:21<04:00,  1.66it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  12%|████▋                                   | 50/425 [00:21<01:33,  4.02it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  18%|███████                                 | 75/425 [00:21<00:47,  7.31it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  24%|█████████▏                             | 100/425 [00:21<00:27, 11.86it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  29%|███████████▍                           | 125/425 [00:21<00:16, 18.01it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  35%|█████████████▊                         | 150/425 [00:21<00:10, 26.28it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  41%|███████████████▉                       | 174/425 [00:21<00:06, 36.38it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  47%|██████████████████▎                    | 199/425 [00:21<00:04, 50.04it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  52%|████████████████████▍                  | 223/425 [00:22<00:03, 65.93it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  58%|██████████████████████▋                | 247/425 [00:22<00:02, 83.37it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  64%|████████████████████████▏             | 271/425 [00:22<00:01, 103.90it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  69%|██████████████████████████▍           | 295/425 [00:22<00:01, 125.05it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  75%|████████████████████████████▌         | 319/425 [00:22<00:00, 145.23it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  81%|██████████████████████████████▋       | 343/425 [00:22<00:00, 164.49it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  86%|████████████████████████████████▊     | 367/425 [00:22<00:00, 180.66it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=val:  92%|██████████████████████████████████▉   | 391/425 [00:22<00:00, 194.40it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "training routine:  20%|█████████████▏                                                    | 1/5 [00:22<01:31, 22.95s/it]\n",
      "\u001b[Ait=train:   0%|                                      | 0/1984 [00:22<00:21, 92.55it/s, acc=3.12, epoch=1, loss=7.36]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Ait=train:  25%|█████████                           | 500/1984 [00:28<00:16, 92.55it/s, acc=5.38, epoch=1, loss=7.84]\n",
      "\u001b[Ait=train:  50%|█████████████████▋                 | 1000/1984 [00:33<00:10, 92.55it/s, acc=6.18, epoch=1, loss=7.74]\n",
      "\u001b[Ait=train:  76%|██████████████████████████▍        | 1500/1984 [00:38<00:05, 92.55it/s, acc=6.62, epoch=1, loss=7.68]\n",
      "\u001b[Ait=train:  83%|████████████████████████████▉      | 1639/1984 [00:40<00:03, 92.55it/s, acc=6.62, epoch=1, loss=7.68]\n",
      "\u001b[Ait=val:   0%|                                        | 0/425 [00:40<00:02, 205.81it/s, acc=6.25, epoch=0, loss=8.41]\n",
      "\u001b[Ait=train: 100%|██████████████████████████████████▉| 1978/1984 [00:43<00:05,  1.06it/s, acc=6.62, epoch=1, loss=7.68]\n",
      "\u001b[Ait=val:   0%|                                           | 0/425 [00:43<00:02, 205.81it/s, acc=0, epoch=1, loss=8.05]\n",
      "training routine:  40%|██████████████████████████▍                                       | 2/5 [00:45<01:08, 22.75s/it]\n",
      "\u001b[Ait=train:   0%|                                      | 0/1984 [00:45<31:08,  1.06it/s, acc=9.38, epoch=2, loss=7.59]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/5]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[Ait=train:  25%|█████████                           | 500/1984 [00:50<23:17,  1.06it/s, acc=8.98, epoch=2, loss=7.27]\n",
      "\u001b[Ait=train:  50%|█████████████████▋                 | 1000/1984 [00:56<15:26,  1.06it/s, acc=8.98, epoch=2, loss=7.22]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
    "\n",
    "    def __init__(self, token_to_idx=None, mask_token=\"<MASK>\", add_unk=True, unk_token=\"<UNK>\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
    "            mask_token (str): the MASK token to add into the Vocabulary; indicates\n",
    "                a position that will not be used in updating the model's parameters\n",
    "            add_unk (bool): a flag that indicates whether to add the UNK token\n",
    "            unk_token (str): the UNK token to add into the Vocabulary\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if token_to_idx is None:\n",
    "            token_to_idx = {}\n",
    "        self._token_to_idx = token_to_idx\n",
    "\n",
    "        self._idx_to_token = {idx: token\n",
    "                              for token, idx in self._token_to_idx.items()}\n",
    "\n",
    "        self._add_unk = add_unk\n",
    "        self._unk_token = unk_token\n",
    "        self._mask_token = mask_token\n",
    "\n",
    "        self.mask_index = self.add_token(self._mask_token)\n",
    "        self.unk_index = -1\n",
    "        if add_unk:\n",
    "            self.unk_index = self.add_token(unk_token)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
    "        return {'token_to_idx': self._token_to_idx,\n",
    "                'add_unk': self._add_unk,\n",
    "                'unk_token': self._unk_token,\n",
    "                'mask_token': self._mask_token}\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
    "        return cls(**contents)\n",
    "\n",
    "    def add_token(self, token):\n",
    "        \"\"\"Update mapping dicts based on the token.\n",
    "\n",
    "        Args:\n",
    "            token (str): the item to add into the Vocabulary\n",
    "        Returns:\n",
    "            index (int): the integer corresponding to the token\n",
    "        \"\"\"\n",
    "        if token in self._token_to_idx:\n",
    "            index = self._token_to_idx[token]\n",
    "        else:\n",
    "            index = len(self._token_to_idx)\n",
    "            self._token_to_idx[token] = index\n",
    "            self._idx_to_token[index] = token\n",
    "        return index\n",
    "\n",
    "    def add_many(self, tokens):\n",
    "        \"\"\"Add a list of tokens into the Vocabulary\n",
    "\n",
    "        Args:\n",
    "            tokens (list): a list of string tokens\n",
    "        Returns:\n",
    "            indices (list): a list of indices corresponding to the tokens\n",
    "        \"\"\"\n",
    "        return [self.add_token(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, token):\n",
    "        \"\"\"Retrieve the index associated with the token\n",
    "          or the UNK index if token isn't present.\n",
    "\n",
    "        Args:\n",
    "            token (str): the token to look up\n",
    "        Returns:\n",
    "            index (int): the index corresponding to the token\n",
    "        Notes:\n",
    "            `unk_index` needs to be >=0 (having been added into the Vocabulary)\n",
    "              for the UNK functionality\n",
    "        \"\"\"\n",
    "        if self.unk_index >= 0:\n",
    "            return self._token_to_idx.get(token, self.unk_index)\n",
    "        else:\n",
    "            return self._token_to_idx[token]\n",
    "\n",
    "    def lookup_index(self, index):\n",
    "        \"\"\"Return the token associated with the index\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to look up\n",
    "        Returns:\n",
    "            token (str): the token corresponding to the index\n",
    "        Raises:\n",
    "            KeyError: if the index is not in the Vocabulary\n",
    "        \"\"\"\n",
    "        if index not in self._idx_to_token:\n",
    "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
    "        return self._idx_to_token[index]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._token_to_idx)\n",
    "\n",
    "\n",
    "class CBOWVectorizer(object):\n",
    "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"\n",
    "\n",
    "    def __init__(self, cbow_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_vocab (Vocabulary): maps words to integers\n",
    "        \"\"\"\n",
    "        self.cbow_vocab = cbow_vocab\n",
    "\n",
    "    def vectorize(self, context, vector_length=-1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            context (str): the string of words separated by a space\n",
    "            vector_length (int): an argument for forcing the length of index vector\n",
    "        \"\"\"\n",
    "\n",
    "        indices = [self.cbow_vocab.lookup_token(token) for token in context.split(' ')]\n",
    "        if vector_length < 0:\n",
    "            vector_length = len(indices)\n",
    "\n",
    "        out_vector = np.zeros(vector_length, dtype=np.int64)\n",
    "        out_vector[:len(indices)] = indices\n",
    "        out_vector[len(indices):] = self.cbow_vocab.mask_index\n",
    "\n",
    "        return out_vector\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe(cls, cbow_df):\n",
    "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
    "\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the target dataset\n",
    "        Returns:\n",
    "            an instance of the CBOWVectorizer\n",
    "        \"\"\"\n",
    "        cbow_vocab = Vocabulary()\n",
    "        for index, row in cbow_df.iterrows():\n",
    "            for token in row.context.split(' '):\n",
    "                cbow_vocab.add_token(token)\n",
    "            cbow_vocab.add_token(row.target)\n",
    "\n",
    "        return cls(cbow_vocab)\n",
    "\n",
    "    @classmethod\n",
    "    def from_serializable(cls, contents):\n",
    "        cbow_vocab = \\\n",
    "            Vocabulary.from_serializable(contents['cbow_vocab'])\n",
    "        return cls(cbow_vocab=cbow_vocab)\n",
    "\n",
    "    def to_serializable(self):\n",
    "        return {'cbow_vocab': self.cbow_vocab.to_serializable()}\n",
    "\n",
    "\n",
    "class CBOWDataset(Dataset):\n",
    "    def __init__(self, cbow_df, vectorizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cbow_df (pandas.DataFrame): the dataset\n",
    "            vectorizer (CBOWVectorizer): vectorizer instatiated from dataset\n",
    "        \"\"\"\n",
    "        self.cbow_df = cbow_df\n",
    "        self._vectorizer = vectorizer\n",
    "\n",
    "        measure_len = lambda context: len(context.split(\" \"))\n",
    "        self._max_seq_length = max(map(measure_len, cbow_df.context))\n",
    "\n",
    "        self.train_df = self.cbow_df[self.cbow_df.split == 'train']\n",
    "        self.train_size = len(self.train_df)\n",
    "\n",
    "        self.val_df = self.cbow_df[self.cbow_df.split == 'val']\n",
    "        self.validation_size = len(self.val_df)\n",
    "\n",
    "        self.test_df = self.cbow_df[self.cbow_df.split == 'test']\n",
    "        self.test_size = len(self.test_df)\n",
    "\n",
    "        self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
    "                             'val': (self.val_df, self.validation_size),\n",
    "                             'test': (self.test_df, self.test_size)}\n",
    "\n",
    "        self.set_split('train')\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_make_vectorizer(cls, cbow_csv):\n",
    "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
    "\n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        train_cbow_df = cbow_df[cbow_df.split == 'train']\n",
    "        return cls(cbow_df, CBOWVectorizer.from_dataframe(train_cbow_df))\n",
    "\n",
    "    @classmethod\n",
    "    def load_dataset_and_load_vectorizer(cls, cbow_csv, vectorizer_filepath):\n",
    "        \"\"\"Load dataset and the corresponding vectorizer.\n",
    "        Used in the case in the vectorizer has been cached for re-use\n",
    "\n",
    "        Args:\n",
    "            cbow_csv (str): location of the dataset\n",
    "            vectorizer_filepath (str): location of the saved vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWDataset\n",
    "        \"\"\"\n",
    "        cbow_df = pd.read_csv(cbow_csv)\n",
    "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
    "        return cls(cbow_df, vectorizer)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectorizer_only(vectorizer_filepath):\n",
    "        \"\"\"a static method for loading the vectorizer from file\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
    "        Returns:\n",
    "            an instance of CBOWVectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath) as fp:\n",
    "            return CBOWVectorizer.from_serializable(json.load(fp))\n",
    "\n",
    "    def save_vectorizer(self, vectorizer_filepath):\n",
    "        \"\"\"saves the vectorizer to disk using json\n",
    "\n",
    "        Args:\n",
    "            vectorizer_filepath (str): the location to save the vectorizer\n",
    "        \"\"\"\n",
    "        with open(vectorizer_filepath, \"w\") as fp:\n",
    "            json.dump(self._vectorizer.to_serializable(), fp)\n",
    "\n",
    "    def get_vectorizer(self):\n",
    "        \"\"\" returns the vectorizer \"\"\"\n",
    "        return self._vectorizer\n",
    "\n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\" selects the splits in the dataset using a column in the dataframe \"\"\"\n",
    "        self._target_split = split\n",
    "        self._target_df, self._target_size = self._lookup_dict[split]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._target_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"the primary entry point method for PyTorch datasets\n",
    "\n",
    "        Args:\n",
    "            index (int): the index to the data point\n",
    "        Returns:\n",
    "            a dictionary holding the data point's features (x_data) and label (y_target)\n",
    "        \"\"\"\n",
    "        row = self._target_df.iloc[index]\n",
    "\n",
    "        context_vector = \\\n",
    "            self._vectorizer.vectorize(row.context, self._max_seq_length)\n",
    "        target_index = self._vectorizer.cbow_vocab.lookup_token(row.target)\n",
    "\n",
    "        return {'x_data': context_vector,\n",
    "                'y_target': target_index}\n",
    "\n",
    "    def get_num_batches(self, batch_size):\n",
    "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
    "\n",
    "        Args:\n",
    "            batch_size (int)\n",
    "        Returns:\n",
    "            number of batches in the dataset\n",
    "        \"\"\"\n",
    "        return len(self) // batch_size\n",
    "\n",
    "\n",
    "def generate_batches(dataset, batch_size, shuffle=True,\n",
    "                     drop_last=True, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    A generator function which wraps the PyTorch DataLoader. It will\n",
    "      ensure each tensor is on the write device location.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
    "                            shuffle=shuffle, drop_last=drop_last)\n",
    "\n",
    "    for data_dict in dataloader:\n",
    "        out_data_dict = {}\n",
    "        for name, tensor in data_dict.items():\n",
    "            out_data_dict[name] = data_dict[name].to(device)\n",
    "        yield out_data_dict\n",
    "\n",
    "\n",
    "class CBOWClassifier(nn.Module):  # Simplified cbow Model\n",
    "    def __init__(self, vocabulary_size, embedding_size, padding_idx=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocabulary_size (int): number of vocabulary items, controls the\n",
    "                number of embeddings and prediction vector size\n",
    "            embedding_size (int): size of the embeddings\n",
    "            padding_idx (int): default 0; Embedding will not use this index\n",
    "        \"\"\"\n",
    "        super(CBOWClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocabulary_size,\n",
    "                                      embedding_dim=embedding_size,\n",
    "                                      padding_idx=padding_idx)\n",
    "        self.fc1 = nn.Linear(in_features=embedding_size,\n",
    "                             out_features=vocabulary_size)\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the classifier\n",
    "\n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor.\n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
    "        \"\"\"\n",
    "        x_embedded_sum = F.dropout(self.embedding(x_in).sum(dim=1), 0.3)\n",
    "        y_out = self.fc1(x_embedded_sum)\n",
    "\n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        return y_out\n",
    "\n",
    "def make_train_state(args):\n",
    "    return {'stop_early': False,\n",
    "            'early_stopping_step': 0,\n",
    "            'early_stopping_best_val': 1e8,\n",
    "            'learning_rate': args.learning_rate,\n",
    "            'epoch_index': 0,\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'val_loss': [],\n",
    "            'val_acc': [],\n",
    "            'test_loss': -1,\n",
    "            'test_acc': -1,\n",
    "            'model_filename': args.model_state_file}\n",
    "\n",
    "def update_train_state(args, model, train_state):\n",
    "    \"\"\"Handle the training state updates.\n",
    "\n",
    "    Components:\n",
    "     - Early Stopping: Prevent overfitting.\n",
    "     - Model Checkpoint: Model is saved if the model is better\n",
    "\n",
    "    :param args: main arguments\n",
    "    :param model: model to train\n",
    "    :param train_state: a dictionary representing the training state values\n",
    "    :returns:\n",
    "        a new train_state\n",
    "    \"\"\"\n",
    "\n",
    "    # Save one model at least\n",
    "    if train_state['epoch_index'] == 0:\n",
    "        torch.save(model.state_dict(), train_state['model_filename'])\n",
    "        train_state['stop_early'] = False\n",
    "\n",
    "    # Save model if performance improved\n",
    "    elif train_state['epoch_index'] >= 1:\n",
    "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
    "\n",
    "        # If loss worsened\n",
    "        if loss_t >= train_state['early_stopping_best_val']:\n",
    "            # Update step\n",
    "            train_state['early_stopping_step'] += 1\n",
    "        # Loss decreased\n",
    "        else:\n",
    "            # Save the best model\n",
    "            if loss_t < train_state['early_stopping_best_val']:\n",
    "                torch.save(model.state_dict(), train_state['model_filename'])\n",
    "\n",
    "            # Reset early stopping step\n",
    "            train_state['early_stopping_step'] = 0\n",
    "\n",
    "        # Stop early ?\n",
    "        train_state['stop_early'] = \\\n",
    "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
    "\n",
    "    return train_state\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "def set_seed_everywhere(seed, cuda):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if cuda:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def handle_dirs(dirpath):\n",
    "    if not os.path.exists(dirpath):\n",
    "        os.makedirs(dirpath)\n",
    "\n",
    "\n",
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    cbow_csv=\"frankenstein_with_splits.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"model_storage/ch5/cbow\",\n",
    "    # Model hyper parameters\n",
    "    embedding_size=50,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    num_epochs=5,\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=32,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    cuda=True,\n",
    "    catch_keyboard_interrupt=True,\n",
    "    reload_from_files=False,\n",
    "    expand_filepaths_to_save_dir=True\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "\n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "\n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "\n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    print(\"Loading dataset and loading vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_load_vectorizer(args.cbow_csv,\n",
    "                                                           args.vectorizer_file)\n",
    "else:\n",
    "    print(\"Loading dataset and creating vectorizer\")\n",
    "    dataset = CBOWDataset.load_dataset_and_make_vectorizer(args.cbow_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "classifier = CBOWClassifier(vocabulary_size=len(vectorizer.cbow_vocab),\n",
    "                            embedding_size=args.embedding_size)\n",
    "\n",
    "classifier = classifier.to(args.device)\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "train_state = make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine',\n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size),\n",
    "                          position=1,\n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size),\n",
    "                        position=1,\n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        print(f\"\\nEpoch [{epoch_index + 1}/{args.num_epochs}]\")\n",
    "        print(\"-\" * 50)\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        classifier.train()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------\n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            if batch_index % 500 == 0:\n",
    "                train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = generate_batches(dataset,\n",
    "                                           batch_size=args.batch_size,\n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        classifier.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "            loss_t = loss.item()\n",
    "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "            # compute the accuracy\n",
    "            acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            if batch_index % 500 == 0:\n",
    "                val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = update_train_state(args=args, model=classifier,\n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "\n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")\n",
    "\n",
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
    "classifier = classifier.to(args.device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = generate_batches(dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   device=args.device)\n",
    "running_loss = 0.\n",
    "running_acc = 0.\n",
    "classifier.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = classifier(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
    "    loss_t = loss.item()\n",
    "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
    "\n",
    "    # compute the accuracy\n",
    "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss\n",
    "train_state['test_acc'] = running_acc\n",
    "\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))\n",
    "\n",
    "\n",
    "def pretty_print(results):\n",
    "    \"\"\"\n",
    "    Pretty print embedding results.\n",
    "    \"\"\"\n",
    "    for item in results:\n",
    "        print(\"...[%.2f] - %s\" % (item[1], item[0]))\n",
    "\n",
    "\n",
    "def get_closest(target_word, word_to_idx, embeddings, n=5):\n",
    "    \"\"\"\n",
    "    Get the n closest\n",
    "    words to your word.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate distances to all other words\n",
    "\n",
    "    word_embedding = embeddings[word_to_idx[target_word.lower()]]\n",
    "    distances = []\n",
    "    for word, index in word_to_idx.items():\n",
    "        if word == \"<MASK>\" or word == target_word:\n",
    "            continue\n",
    "        distances.append((word, torch.dist(word_embedding, embeddings[index])))\n",
    "\n",
    "    results = sorted(distances, key=lambda x: x[1])[1:n + 2]\n",
    "    return results\n",
    "\n",
    "word = 'monster'\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "pretty_print(get_closest(word, word_to_idx, embeddings, n=5))\n",
    "\n",
    "target_words = ['frankenstein', 'monster', 'science', 'sickness', 'lonely', 'happy']\n",
    "\n",
    "embeddings = classifier.embedding.weight.data\n",
    "word_to_idx = vectorizer.cbow_vocab._token_to_idx\n",
    "\n",
    "for target_word in target_words:\n",
    "    print(f\"======={target_word}=======\")\n",
    "    if target_word not in word_to_idx:\n",
    "        print(\"Not in vocabulary\")\n",
    "        continue\n",
    "    pretty_print(get_closest(target_word, word_to_idx, embeddings, n=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f4754-22d7-4511-be73-7e6716776821",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
